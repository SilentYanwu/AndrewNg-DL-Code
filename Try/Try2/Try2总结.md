在Try2中从*tensorflow*的架构更改为*pytorch*
在Chatgpt的帮助下，更改了标签数据（而非独热码），确定数值是LongTensor，更改了评估函数
最终实现了大约**80%**的正确率
在Try1中，我们已经使用了如batch norm，dropout正则化等等未在这个项目Try2中实现的代码，
但是即使在Try2中实现了DNN的各种优化算法，使其正确率上升，但是在面对本人出的测试集的时候，正确率还是非常低。
因此是时候引入**CNN**了。

我们最后，把DNN优化的方法和流程都列举一遍，由于Try1已经做的很好了，我们就针对Try1开始梳理：
下面是对 Try1 的所有技巧和实现要点的系统总结👇：

## 🧠 Try1 技巧总结：从传统 DNN 到现代深度学习训练体系

---

### 🩵 一、数据与预处理技巧（Data Handling）

| 技巧                    | 作用                          | Try1 中的实现                                      |
| --------------------- | --------------------------- | ---------------------------------------------- |
| ✅ **路径修复（fix_paths）** | 保证脚本在任意路径都能正确加载数据           | `fix_paths()` 自动切换工作目录                         |
| ✅ **HDF5 数据加载**       | 高效读取大规模 numpy 数据            | 使用 `h5py.File()` 读取 `.h5` 数据集                  |
| ✅ **自定义 Dataset 类**   | 灵活定义图像读取与增强流程               | `CatDataset` 实现 `__getitem__` 和 `__len__`      |
| ✅ **OpenCV + RGB 转换** | 图像通道从 BGR→RGB（PyTorch 预期格式） | `cv2.cvtColor(image, cv2.COLOR_BGR2RGB)`       |
| ✅ **float32 标签**      | 兼容 `BCELoss` 需要浮点型标签        | `torch.tensor(label, dtype=torch.float32)`     |
| ✅ **mini-batch 加载器**  | 自动打乱与批处理                    | `DataLoader(..., batch_size=32, shuffle=True)` |

💡 **效果：**
让数据预处理标准化、GPU 加载稳定、与 PyTorch 的训练流程无缝衔接。

---

### 🧩 二、数据增强（Data Augmentation）

| 技巧                  | 作用                 | Try1 中的实现                                |
| ------------------- | ------------------ | ---------------------------------------- |
| ✅ **随机水平翻转**        | 模拟左右对称姿态           | `transforms.RandomHorizontalFlip(p=0.5)` |
| ✅ **随机旋转**          | 增强模型对方向变化的鲁棒性      | `transforms.RandomRotation(15)`          |
| ✅ **亮度对比度扰动**       | 模拟光照变化             | `transforms.ColorJitter()`               |
| ✅ **ToTensor() 转换** | 将图像标准化为 0-1 Tensor | `transforms.ToTensor()`                  |

💡 **效果：**
减轻过拟合，让网络更具泛化性，尤其对小样本任务非常重要。

---

### 🧱 三、模型结构优化（Network Architecture）

| 技巧                        | 作用                  | Try1 中的实现                              |
| ------------------------- | ------------------- | -------------------------------------- |
| ✅ **模块化网络设计**             | 便于快速调整层结构           | `layer_dims = [64*64*3, 64, 32, 8, 1]` |
| ✅ **Batch Normalization** | 稳定激活分布，加快收敛         | `nn.BatchNorm1d(layer_dims[i])`        |
| ✅ **ReLU 激活函数**           | 非线性增强，避免梯度消失        | `nn.ReLU()`                            |
| ✅ **Dropout 正则化**         | 随机丢弃神经元，防止过拟合       | `nn.Dropout(p=0.3)`                    |
| ✅ **Sigmoid 输出层**         | 将输出转为 0-1 概率（适合二分类） | `torch.sigmoid(out)`                   |

💡 **效果：**
这是现代 DNN 的核心套路——
BN + ReLU + Dropout 三连击，有效稳定训练并增强泛化能力。

---

### ⚙️ 四、训练策略（Training Strategy）

| 技巧                     | 作用         | Try1 中的实现                                                    |
| ---------------------- | ---------- | ------------------------------------------------------------ |
| ✅ **Adam 优化器**         | 自适应学习率收敛快  | `optim.Adam(lr=0.0006, weight_decay=1e-4)`                   |
| ✅ **L2 正则化（权重衰减）**     | 抑制过拟合      | `weight_decay=1e-4`                                          |
| ✅ **mini-batch 梯度下降**  | 减少波动、加速训练  | `for imgs, labels in train_loader:`                          |
| ✅ **BCELoss 损失函数**     | 适合二分类任务    | `criterion = nn.BCELoss()`                                   |
| ✅ **梯度清零 + 反向传播 + 更新** | 经典训练三步     | `optimizer.zero_grad() → loss.backward() → optimizer.step()` |
| ✅ **验证集监控**            | 防止过拟合并追踪性能 | 每轮 `val_loss` 监控与可视化                                         |
| ✅ **绘制损失曲线**           | 直观评估训练是否稳定 | Matplotlib 可视化训练/验证 loss                                     |

💡 **效果：**
让训练过程更平滑、稳定、易于观察，
Adam + BN + Dropout 通常是入门最强 baseline。

---

### 📊 五、模型评估与保存（Evaluation & Save）

| 技巧            | 作用                 | Try1 中的实现                         |
| ------------- | ------------------ | --------------------------------- |
| ✅ **模型评估模式**  | 禁用 Dropout / BN 更新 | `model.eval()`                    |
| ✅ **禁用梯度计算**  | 节省显存与加速推理          | `with torch.no_grad():`           |
| ✅ **预测阈值处理**  | Sigmoid 概率转 0/1 标签 | `(outputs > 0.5).float()`         |
| ✅ **准确率计算**   | 快速评估性能             | `(preds == labels).sum() / total` |
| ✅ **动态保存文件名** | 自动带准确率后缀           | `"cat_model_{accc:.2f}%.pth"`     |

💡 **效果：**
形成完整的训练→验证→测试→保存的闭环，
每次运行都能保留最优模型结果。

---

### 🧮 六、DNN 优化总结逻辑（核心思想）

| 类别        | 优化目的   | 技术                          |
| --------- | ------ | --------------------------- |
| **初始化层面** | 让模型能学到 | Xavier/He 初始化（PyTorch 自动包含） |
| **正则化层面** | 防止过拟合  | Dropout、L2、数据增强             |
| **归一化层面** | 加快收敛   | BatchNorm                   |
| **优化器层面** | 稳定下降   | Adam（含权重衰减）                 |
| **结构层面**  | 表达能力   | ReLU + 多层隐藏层                |
| **数据层面**  | 泛化能力   | Augmentation                |
| **评估层面**  | 防止盲目训练 | 验证集 + 损失曲线                  |

